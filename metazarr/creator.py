"""
creator.py
åŠŸèƒ½ä¸€ï¼šåŸå§‹ (GRIB / NetCDF / HDF) âœ Zarr
        æˆ–ç›´æ¥ç™»è®°ç°æˆ Zarr ç›®å½•é›†åˆ
"""
from __future__ import annotations
import time, shutil
from pathlib import Path
from typing import List, Dict
import shutil
import os
import dask
from dask.diagnostics import ProgressBar
import xarray as xr

from .config import *
from .utils import (
    log,
    ensure_empty_dir,
    scan_files,
    load_catalog,
    save_catalog,
    timestamp,
)
from .exceptions import ValidationError, ConversionError

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ å†…éƒ¨å°å·¥å…· â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _open_raw(path: Path, fmt: RawFormat) -> xr.Dataset:
    if fmt is RawFormat.GRIB:
        return xr.open_dataset(path, engine="cfgrib", chunks={}, backend_kwargs={"indexpath": ""})
    if fmt is RawFormat.NETCDF:
        return xr.open_dataset(path, engine="netcdf4", chunks={})
    if fmt is RawFormat.HDF:
        return xr.open_dataset(path, engine="h5netcdf", chunks={})
    raise ConversionError(f"ä¸æ”¯æŒçš„è¾“å…¥æ ¼å¼ {fmt}")

def _auto_chunks(ds: xr.Dataset) -> dict:
    """ç®€å• heuristicï¼štime/step=1ï¼Œlatâ‰¤180ï¼Œlonâ‰¤360"""
    chunks = {}
    for dim, n in ds.sizes.items():
        if dim in {"time", "step"}:
            chunks[dim] = 1
        elif dim.lower().startswith("lat"):
            chunks[dim] = min(n, 180)
        elif dim.lower().startswith("lon"):
            chunks[dim] = min(n, 360)
    return chunks

def _write_zarr(ds: xr.Dataset, target: Path, consolidate: bool = True):
    t0 = time.time()
    ds.chunk(_auto_chunks(ds)).to_zarr(
        str(target),
        mode="w",
        compute=True,
        consolidated=consolidate,
        encoding={v: {"compressor": DEFAULT_COMPRESSOR} for v in ds.data_vars},
    )
    log.info("âœ… å†™å…¥ %s (%.1fs)", target.name, time.time() - t0)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ catalog å†™å…¥ç»Ÿä¸€å‡½æ•° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _update_catalog(
    *,
    name: str,
    description: str,
    data_kind: DataKind,
    path: Path,
    spatial_res,
    temporal_res,
    coords: List[str],
    variables: List[str],
    org_mode: OrgMode,
    allow_update: bool,
    start_time: str | None = None, 
    end_time: str | None = None, 
):
    cat = load_catalog()
    cat[name] = {
        "name":         name,
        "kind":         data_kind.value,
        "format":       "zarr",
        "description":  description,
        "path":         str(path),
        "spatial_res":  spatial_res,
        "temporal_res": temporal_res,
        "start_time":   start_time,     
        "end_time":     end_time,      
        "coords":       coords,
        "variables":    variables,
        "org_mode":     org_mode.value,
        "created":      timestamp(),        # åŒ—äº¬æ—¶é—´ (+08:00)
        "allow_update": allow_update,
    }
    save_catalog(cat)
    log.info("ğŸ“š Catalog å·²æ›´æ–° â†’ %s", name)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ä¸»å…¥å£ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def create_dataset(
    *,
    data_kind: DataKind,
    raw_format: RawFormat,
    description: str,
    src_paths: List[str | Path],
    dst_path: str | Path | None = None,
    org_mode: OrgMode,
    name: str,
    spatial_res: tuple[int, int, int] | None = None,
    temporal_res: str | None = None,
    allow_update: bool = False,
) -> Path:
    """
    â€¢ raw_format âˆˆ {GRIB, NETCDF, HDF}  â†’ è½¬ Zarr
    â€¢ raw_format == ZARR                â†’ ä»…ç™»è®°ç›®å½•ä¸‹æ‰€æœ‰ *.zarrï¼ˆä¸å¤åˆ¶ï¼‰
    è¿”å›æ ¹ç›®å½• Pathã€‚
    """
    cat = load_catalog()
    # 1) åŒåæ•°æ®é›†å·²å­˜åœ¨
    if name in cat:
        raise ValidationError(
            f"æ•°æ®é›†å {name!r} å·²å­˜åœ¨ã€‚\n"
            "å¦‚æœæƒ³å‘è¯¥æ•°æ®é›†è¿½åŠ ï¼Œè¯·ä½¿ç”¨ append_dataset()ï¼›"
            "å¦‚æœæƒ³åˆ›å»ºæ–°é›†ï¼Œè¯·æ¢ä¸€ä¸ª nameã€‚"
        )
    # 2) ç›®å½•è¢«å…¶ä»–æ•°æ®é›†å ç”¨
    for meta in cat.values():
        if Path(meta["path"]).resolve() == Path(dst_path or src_paths[0]).resolve():
            raise ValidationError(
                f"ç›®å½• {meta['path']} å·²è¢«æ•°æ®é›† {meta['name']} ä½¿ç”¨ã€‚\n"
                "è¯·æŒ‡å®šä¸€ä¸ªæ–°çš„ dst_pathï¼Œæˆ–ä½¿ç”¨ append_dataset() è¿½åŠ ã€‚"
            )
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ZARR: åªç™»è®°ç°æˆæ•°æ® â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    if raw_format is RawFormat.ZARR:
        root = Path(dst_path).resolve() if dst_path else Path(src_paths[0]).resolve()
        if not root.is_dir():
            raise ValidationError(f"{root} ä¸æ˜¯æœ‰æ•ˆç›®å½•")
        zarr_stores = sorted(root.glob("*.zarr"))
        if not zarr_stores:
            raise ValidationError(f"{root} ä¸‹æœªå‘ç° *.zarr ç›®å½•")

        ds_all = xr.open_mfdataset(
            [str(p) for p in zarr_stores],
            engine="zarr",
            concat_dim="time",
            combine="nested",
            coords="minimal",
            parallel=True,
            backend_kwargs={"consolidated": False},
        )
        time_coord = "valid_time" if "valid_time" in ds_all.coords else (
                     "time"       if "time" in ds_all.coords else None)
        start_t = end_t = None
        if time_coord:
            times = ds_all[time_coord].values
            start_t = str(times.min())[:19]      # yyyy-mm-ddThh:mm:ss
            end_t   = str(times.max())[:19]
        _update_catalog(
            name=name,
            description=description,
            data_kind=data_kind,
            path=root,
            spatial_res=spatial_res,
            temporal_res=temporal_res,
            coords=list(ds_all.coords),
            variables=list(ds_all.data_vars),
            org_mode=org_mode,
            allow_update=allow_update,
            start_time=start_t,
            end_time=end_t,
        )
        log.info("âœ… å·²ç™»è®° Zarr æ•°æ®é›† %s (å…± %d ä¸ª .zarr)", name, len(zarr_stores))
        return root

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ GRIB/NetCDF/HDF âœ Zarr è½¬æ¢ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    dst_path = Path(dst_path) if dst_path else Path(f"./{name}_zarr")
    ensure_empty_dir(dst_path)

    log.info("æ‰«æåŸå§‹æ–‡ä»¶â€¦")
    files = list(scan_files([Path(p) for p in src_paths]))
    if not files:
        raise ValidationError("æœªæ‰¾åˆ°ä»»ä½•åŸå§‹æ–‡ä»¶")

    # è§£ææ–‡ä»¶å â†’ ä»¥ 10 ä½èµ·æŠ¥æ—¶ (YYYYMMDDHH) åˆ†ç»„
    parsed: Dict[str, dict] = {}
    for f in files:
        m = FILENAME_RE.match(f.name)
        if not m:
            log.warning("è·³è¿‡ä¸ç¬¦åˆå‘½åè§„åˆ™: %s", f)
            continue
        gd = m.groupdict()
        cycle_key = gd["datetime"]             # YYYYMMDDHH
        parsed.setdefault(cycle_key, {}).setdefault(gd["var"] or "var", []).append(
            dict(path=f, step=gd.get("step"))
        )
    all_cycles = sorted(parsed.keys())     # ['2024070100', '2024070200', ...] æˆ– ['20240701', '20240702', ...]
    start_cycle = all_cycles[0]
    end_cycle   = all_cycles[-1]
    if not parsed:
        raise ValidationError("æ²¡æœ‰æ–‡ä»¶ç¬¦åˆå‘½åè§„èŒƒ")

    tasks = []
    for cycle, var_map in parsed.items():
        pieces_each_var = []
        for var, lst in var_map.items():
            subpieces = []
            for item in sorted(lst, key=lambda x: x["step"] or "000"):
                ds = _open_raw(item["path"], raw_format)
                if data_kind is DataKind.FORECAST:
                    ds = ds.expand_dims(step=[int(item["step"] or 0)])
                subpieces.append(ds)
            concat_dim = "step" if data_kind is DataKind.FORECAST else "time"
            pieces_each_var.append(xr.concat(subpieces, dim=concat_dim))
        ds_cycle = xr.merge(pieces_each_var)

        tgt = dst_path / (f"{cycle}.zarr" if allow_update else ".")
        if tgt.suffix != ".zarr":
            tgt = dst_path
        else:
            tgt.mkdir(parents=True, exist_ok=True)

        tasks.append(dask.delayed(_write_zarr)(ds_cycle, tgt, consolidate=not allow_update))

    with ProgressBar():
        dask.compute(*tasks)

    _update_catalog(
        name=name,
        description=description,
        data_kind=data_kind,
        path=dst_path,
        spatial_res=spatial_res,
        temporal_res=temporal_res,
        coords=list(ds_cycle.coords),
        variables=list(ds_cycle.data_vars),
        org_mode=org_mode,
        allow_update=allow_update,
        start_time=start_cycle,      # â† æ–°å¢
        end_time=end_cycle,          # â† æ–°å¢
    )
    log.info("âœ… æ•°æ®é›† %s åˆ›å»ºå®Œæˆ @ %s", name, dst_path)
    return dst_path

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ è¿½åŠ å‡½æ•° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def append_dataset(name: str, new_files: List[str | Path]):
    meta = load_catalog().get(name)
    if not meta or not meta["allow_update"]:
        raise ValidationError(f"æ•°æ®é›† {name} ä¸å­˜åœ¨æˆ–æœªå¼€å¯å¯è¿½åŠ æ¨¡å¼")

    dst = Path(meta["path"])
    fmt = (
        RawFormat.ZARR
        if all(Path(p).suffix == ".zarr" for p in new_files)
        else RawFormat.GRIB
        if any(str(p).lower().endswith(".grb") for p in new_files)
        else RawFormat.NETCDF
    )

    create_dataset(
        data_kind=DataKind(meta["kind"]),
        raw_format=fmt,
        description=meta["description"],
        src_paths=new_files,
        dst_path=dst,
        org_mode=OrgMode(meta["org_mode"]),
        name=name,
        allow_update=True,
    )
def delete_dataset(name: str, *, remove_files: bool = False):
    """
    åˆ é™¤ catalog ä¸­çš„æŸä¸ªæ•°æ®é›†ã€‚
    å‚æ•°
    -----
    name          æ•°æ®é›†æ³¨å†Œå
    remove_files  True â‡’ åŒæ—¶åˆ é™¤ catalog[path] æ‰€æŒ‡ Zarr ç›®å½•
    """
    cat = load_catalog()
    meta = cat.pop(name, None)
    if meta is None:
        raise ValidationError(f"æ•°æ®é›† {name} ä¸å­˜åœ¨")

    # 1) åˆ é™¤ç£ç›˜æ•°æ®ï¼ˆå¯é€‰ï¼‰
    if remove_files:
        p = Path(meta["path"])
        if p.exists():
            log.info("ğŸ—‘ï¸  æ­£åœ¨åˆ é™¤ç£ç›˜æ•°æ® %s â€¦", p)
            # æ—¢å¯èƒ½æ˜¯å•ä¸ª .zarrï¼Œä¹Ÿå¯èƒ½æ˜¯ä¸€ä¸ªç›®å½•åŒ…å«å¤š .zarr
            if p.is_dir():
                shutil.rmtree(p)
            else:
                os.remove(p)

    # 2) å†™å› catalog
    save_catalog(cat)
    log.info("âœ… å·²åˆ é™¤æ•°æ®é›† %sï¼ˆremove_files=%sï¼‰", name, remove_files)